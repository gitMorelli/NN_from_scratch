#ifndef FUNCTIONS_NEW_H_INCLUDED
#define FUNCTIONS_NEW_H_INCLUDED
#endif // FUNCTIONS_NEW_H_INCLUDED
/*
In summary, the provided code is a common practice in C and C++ to prevent 
multiple inclusions of the same header file, ensuring that the declarations within 
the header file are only included once in the program.
*/

#include <math.h>
#include "graphics.h"

#define MAXEXE 100 // max # of examples in TS
#define MAXINP 100 // max # of input units
#define MAXHID 100 // max # of hidden units
#define MAXOUT 10 // max # of output units
#define ETA0 0.5 // default learning rate
#define MU0 0.5 // de
#define PI 3.1415926535

static int te[MAXEXE][MAXINP]; // training examples
static int to[MAXEXE][MAXOUT]; // training output labels
static int ve[MAXEXE][MAXINP]; // validation examples
static int vo[MAXEXE][MAXOUT]; // validation output labels
static int map[MAXEXE]; // shuffled TS vector
static float wo[MAXOUT][MAXHID]; // weights of output units
static float wh[MAXHID][MAXINP]; // weights of hidden units
static float bo[MAXOUT]; // biases of output units
static float bh[MAXHID]; // biases of hidden units
static float dwo[MAXOUT][MAXHID]; // output weight variations
static float dwh[MAXHID][MAXINP]; // hidden weight variations
static float dbo[MAXOUT]; // output biases variations
static float dbh[MAXHID]; // hidden biases variations
static float x[MAXINP]; // input vector
static float h[MAXHID]; // hidden vector
static float y[MAXOUT]; //output vector

static int inp; // number of input units
static int hid; // number of hidden units
static int out; // number of output units
static int nex; // number of training examples
static int vex; // number of validation examples
//static int mbs; // size of mini-batch 
static float eta = ETA0; // learning
static float mu = MU0; //momentum

float rand_normal(float mu, float sigma);
float sigmoid(float a);
void bp_define_net(int ni, int nh, int no);
void bp_reset_weights();
void bp_set_learning_rate(float lr);
void bp_set_momentum(float m);
void bp_set_input(float *v);
void bp_set_input_neuron(int i, float v);
void bp_set_ts_input(int k);
void bp_set_vs_input(int k);
void bp_compute_output();
float bp_compute_ts_error(int *tv);
void bp_reset_dw();
void bp_accumulate_dw(int k);
void bp_compute_dw(int k);
void bp_update_weights();

int bp_load_ts(char *name);
int bp_load_vs(char *name);
void bp_shuffle_ts();
int bp_get_shuffled_sample(int i);
void bp_get_units(int *i, int *h, int *o);
float bp_get_learning_rate();
float bp_get_momentum();
float bp_get_input_neuron(int i);
float bp_get_hidden_neuron(int i);
float bp_get_output_neuron(int i);
float bp_get_ts_sample(int k, int i);
float bp_get_vs_sample(int k, int i);
float bp_get_wo(int j, int i);
float bp_get_wh(int h, int i);
float bp_learn_sample(int k);
float bp_learn_batch();
float bp_learn_mb(int b, int mbs);
float bp_learn_minibatch(int b, int mbs, int maxepoch);



float rand_normal(float mu, float sigma)
{
float x, y, r; // generated by Box-Muller algorithm
    x = (float)rand()/RAND_MAX;
    y = (float)rand()/RAND_MAX;
    r = cos(2*PI*y)*sqrt(-2.*log(x));
    return mu + sigma*r;
}

float sigmoid(float a){
float y;
    y = 1.0 / (1.0 + exp(-a));
    return y;
}

void bp_define_net(int ni, int nh, int no)
{
inp = ni; hid = nh; out = no;
eta = ETA0; mu = MU0;
}

void bp_reset_weights()
{
int i, j;
float sigma;
    sigma = sqrt(1.0/inp);
    for (j=0; j<hid; j++) { // init hidden layer
        for (i=0; i<inp; i++)
            wh[j][i] = rand_normal(0, sigma);
        bh[j] = rand_normal(0, sigma);
    }
    sigma = sqrt(1.0/hid);
    for (j=0; j<out; j++) { // init output layer
        for (i=0; i<hid; i++)
            wo[j][i] = rand_normal(0, sigma);
        bo[j] = rand_normal(0, sigma);
    }
}


void bp_set_input(float *v)
{
int i;
    for (i=0; i<inp; i++)
        x[i] = v[i];
}

void bp_set_ts_input(int k)
{
int i;
    for (i=0; i<inp; i++)
        x[i] = te[k][i];
}

void bp_compute_output()
{
int i, j;
float sum;
    for (j=0; j<hid; j++) { // for each hidden unit
        sum = 0.0;
        for (i=0; i<inp; i++)
            sum += wh[j][i] * x[i];
        h[j] = sigmoid(sum + bh[j]);
    }
    for (j=0; j<out; j++) { // for each output unit
        sum = 0.0;
        for (i=0; i<hid; i++)
            sum += wo[j][i] * h[i];
        y[j] = sigmoid(sum + bo[j]);
    }
}

float bp_compute_ts_error(int k)
{
int j;
float d, err = 0.0;
    for (j=0; j<out; j++) {
        d = to[k][j] - y[j];
        err += 0.5*d*d;
    }
    return err;
}

//---------------------------------------------------------
// Compute weight variations for training sample k
//---------------------------------------------------------
void bp_compute_dw(int k)
{
int i, j;
float deltao[MAXOUT]; // delta error for output units
float deltah; // delta error for hidden units
float sum;
    for (j=0; j<out; j++) {
        deltao[j] = y[j]*(1-y[j])*(to[k][j] - y[j]);
        dbo[j] = eta*deltao[j] + mu*dbo[j]; // bias
        for (i=0; i<hid; i++) // weights
            dwo[j][i] = eta*deltao[j]*h[i] + mu*dwo[j][i];
    }
    for (i=0; i<hid; i++) { // hidden weights
        sum = 0.0; // backpropagation step
        for (j=0; j<out; j++)
            sum += wo[j][i] * deltao[j];
        deltah = h[i]*(1-h[i])*sum;
        dbh[i] = eta*deltah + mu*dbh[i]; // bias
        for (j=0; j<inp; j++) // weights
            dwh[i][j] = eta*deltah*x[j] + mu*dwh[i][j];
    }
}

void bp_update_weights()
{
int h, i, j;
    for (j=0; j<out; j++) {
        bo[j] += dbo[j]; // out bias
        for (i=0; i<hid; i++) // out weights
            wo[j][i] += dwo[j][i];
    }
    for (i=0; i<hid; i++) {
        bh[i] += dbh[i]; // hid bias
        for (h=0; h<inp; h++) // hid weights
            wh[i][h] += dwh[i][h];
    }
}

float bp_learn_example(int k)
{
float err;
    bp_set_ts_input(k);
    bp_compute_output();
    err = bp_compute_ts_error(k);
    bp_compute_dw(k);
    bp_update_weights();
    return err;
}

void bp_learn_online1(float eps, int maxiter)
{
int k = 0; // example index
int iter=0;
float gerr;
    do{
        iter++;
        gerr=bp_learn_example(k);
        display_error(iter,gerr);
        display_net();
        display_weights();
        k = (k + 1) % nex; // next example
    }while ((iter<maxiter)&& (gerr>eps));
}

void bp_learn_online2(float eps, int maxiter)
{
int k = 0; // example index
int iter = 0; // iteration counter
float gerr; // global error
    do {
        iter++;
        k = rand() % nex;
        gerr = bp_learn_example(k);
        display_error(iter, gerr);
        display_net();
        display_weights();
    } while ((iter < maxiter) && (gerr > eps));
}

void bp_learn_online3(float eps, int maxiter)
{
int k, i = 0; // example index
int iter = 0; // iteration counter
float gerr; // global error
    do {
        iter++;
        if (i == 0) bp_shuffle_ts();
        k = map[i];
        gerr = bp_learn_example(k);
        display_error(iter, gerr);
        display_net();
        display_weights();
        i = (i + 1) % nex;
    } while ((iter < maxiter) && (gerr > eps));
}

float bp_learn_mb(int b, int mbs){
int i, j, k;
float Eg = 0.0;
    bp_reset_dw();
    bp_shuffle_ts();
    for (i=0; i<mbs; i++) { //mbs is the number of elements in a mini-batch
        j = i + b*mbs; // compute mb index
        k = map[j]; // get sample index
        bp_set_ts_input(k);
        bp_compute_output();
        Eg += bp_compute_ts_error(k);
        bp_accumulate_dw(k);
    }
    bp_update_weights();
    return (Eg/mbs);
}

 // b = minibatch index

void bp_reset_dw()
{
int i, j;
    for (j=0; j<out; j++) { // output weights
        dbo[j] = 0; // bias
        for (i=0; i<hid; i++) // weights
            dwo[j][i] = 0;
    }
    for (j=0; j<hid; j++) { // hidden weights
        dbh[j] = 0; // bias
        for (i=0; i<inp; i++) // weights
            dwh[j][i] = 0;
    }
}

void bp_shuffle_ts()
{
int i, k, temp;
static int first;
    if (first == 0) {
        for (k=0; k<nex; k++) map[k] = k;
        first = 1;
    }
    for (k=0; k<nex; k++) {
        i = rand() % nex;
        temp = map[k];
        map[k] = map[i];
        map[i] = temp;
    }
}

void bp_accumulate_dw(int k)
{
int i, j;
float deltao[MAXOUT]; // delta error for output units
float deltah; // delta error for hidden units
float sum;
    for (j=0; j<out; j++) { // output weights
        deltao[j] = y[j]*(1-y[j])*(to[k][j] - y[j]);
        dbo[j] += eta*deltao[j] + mu*dbo[j]; // bias
        for (i=0; i<hid; i++) // weights
            dwo[j][i] += eta*deltao[j]*h[i] + mu*dwo[j][i];
    }
    for (i=0; i<hid; i++) { // hidden weights
        sum = 0.0; // backpropagation step
        for (j=0; j<out; j++)
            sum += wo[j][i] * deltao[j];
        deltah = h[i]*(1-h[i])*sum;
        dbh[i] += eta*deltah + mu*dbh[i]; // bias
        for (j=0; j<inp; j++) // weights
            dwh[i][j] += eta*deltah*x[j] + mu*dwh[i][j];
    }
}

int bp_learn_minibatch(float eps, int mbs, int maxepoch)
{
int b, i, j, k;
int iter = 0; // iteration counter
int epoch = 0; // epoch counter
float gerr; // global error
    do {
        bp_shuffle_ts();
        for (b=0; b<nex/mbs; b++) {
            gerr = bp_learn_mb(b,mbs);
            bp_display_error(iter, gerr);
            iter++;
        }
        epoch++;
    }while ((gerr > eps) && (epoch < maxepoch));
    return epoch;
}

float bp_learn_ts()
{
int i, k;
float Eg = 0.0;
    bp_reset_dw();
    bp_shuffle_ts();
    for (i=0; i<nex; i++) {
        k = map[i];
        bp_set_ts_input(k);
        bp_compute_output();
        Eg += bp_compute_ts_error(k);
        bp_accumulate_dw(k);
    }
    bp_update_weights();
    return (Eg/nex);
}


float bp_learn_batch(float eps, int maxepoch)
{
int epoch = 0; // epoch counter
float gerr = 0.0; // global error
    do {
        gerr = bp_learn_ts();
        epoch++;
        display_error(epoch, gerr);
        display_net();
        display_weights();
    } while ((epoch < maxepoch) && (gerr > eps));
}
